import os
import json
import time
import requests
import threading
import argparse
import sys
import math
import hashlib
from filechunkio import FileChunkIO
from concurrent.futures import ThreadPoolExecutor
from ks3.connection import Connection
from ks3.multipart import PartInfo
# TODO:æ‰“åŒ…æ”¾å¼€
from robot_data_uploader import config
# NOTE:å¼€å‘è°ƒè¯•
# import config
from tqdm import tqdm
from io import BytesIO
from colorama import init, Fore, Style
import pyfiglet
from pyfiglet import Figlet
# åˆå§‹åŒ–é¢œè‰²è¾“å‡º
init(autoreset=True)

__version__ = "1.0.0"
__author__ = "DataPlatform"
# __license__ = "Apache-2.0"
__repo__ = "https://gitee.com/baai-data/baai-eai-datasuite.git"
__description__ = "æœºå™¨æ•°æ®ä¸Šä¼ å·¥å…·,æ”¯æŒæ–­ç‚¹ç»­ä¼ ã€è¿›åº¦æ˜¾ç¤ºå’Œæ–‡ä»¶è¿‡æ»¤"
__help__ = "å¦‚é‡é—®é¢˜è¯·åŠæ—¶æŠ¥å‘Šåé¦ˆ: dataplatform@baai.ac.cn"

# ä»¥ä¸‹æ˜¯åŸclient/uploader.pyçš„å…¶ä½™å†…å®¹ï¼Œä¿æŒä¸å˜
def print_header():
    """æ˜¾ç¤ºå·¥å…·å¤´ä¿¡æ¯"""
    f = Figlet(font='slant')
    print(Fore.CYAN + f.renderText('Robot Data Uploader'))
    
    print(f"{Fore.YELLOW}â– Version{Style.RESET_ALL}: {__version__}")
    print(f"{Fore.YELLOW}â– Author{Style.RESET_ALL}:  {__author__}")
    # print(f"{Fore.YELLOW}â– License{Style.RESET_ALL}: {__license__}")
    print(f"{Fore.YELLOW}â– Source{Style.RESET_ALL}:  {__repo__}")
    print("\n" + "-" * 80)

def show_banner():
    """æ˜¾ç¤ºç¨‹åºæ¨ªå¹…"""
    banner = pyfiglet.figlet_format("BAAI Robot Data Uploader", font="slant")
    print(f"{Fore.CYAN}{banner}")
    print(f"{Fore.CYAN}{'=' * 60}")
    print(f"{Fore.YELLOW}â– Description{Style.RESET_ALL}: {__description__}")
    print(f"{Fore.YELLOW}â– Version{Style.RESET_ALL}: {__version__}")
    print(f"{Fore.YELLOW}â– Author{Style.RESET_ALL}:  {__author__}")
    print(f"{Fore.YELLOW}â– Help{Style.RESET_ALL}:  {__help__}")

    # print(f"{Fore.YELLOW}ç›®æ ‡å­˜å‚¨: {config.BUCKET_NAME}")
    print(f"{Fore.CYAN}{'=' * 60}\n")

from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich import box

console = Console()

def show_menu(current_filters, auth_method, max_workers):
    # èœå•é¢æ¿
    panel = Panel(
        f"[bold cyan]â‹™ æ™ºæºæœºå™¨äººæ•°æ®ä¼ è¾“ç®¡ç†å™¨ v1.0.0[/]\n[dim]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/]\n[dim]å½“å‰çº¿è·¯: {config.ENDPOINT_TYPE} ({config.ENDPOINT})[/]",
        title="[bold yellow]âˆ ä¸»èœå•[/]",
        subtitle="[dim italic]â†‘â†“ é€‰æ‹©ï¼Œâ†© ç¡®è®¤[/]",
        border_style="bright_blue"
    )
    console.print(panel)

    # é€‰é¡¹è¡¨æ ¼
    table = Table.grid(padding=(0, 2))
    table.add_column(style="bold cyan", width=2)  # åºå·åˆ—
    table.add_column(style="bold white")          # åŠŸèƒ½åˆ—
    table.add_column(style="dim yellow")          # çŠ¶æ€åˆ—
    # current_filters = [ "*.txt", "*.csv", "*.json", "*.dat" , "*.tar", "*.png"]
    # auth_method = "STS"
    table.add_row("1", "ğŸ“¤ ä¸Šä¼ å•ä¸ªæ–‡ä»¶", "[è¯·é€‰æ‹©å•ä¸ªæ–‡ä»¶]")
    table.add_row("2", "ğŸ“‚ ä¸Šä¼ ç›®å½•", "[é€’å½’ä¸Šä¼ æ‰€æœ‰åŒ¹é…æ–‡ä»¶]")
    table.add_row("3", "ğŸ›¡ï¸ è®¾ç½®æ–‡ä»¶è¿‡æ»¤å™¨", f"[å½“å‰: [bold green]{current_filters}[/]]")
    table.add_row("4", "ğŸ”‘ åˆ‡æ¢è®¤è¯æ–¹å¼", f"[å½“å‰: [bold magenta]{auth_method}[/]]")
    table.add_row("5", "âš™ï¸ è®¾ç½®ä¸Šä¼ çº¿ç¨‹æ•°", f"[å½“å‰: [bold blue]{max_workers}[/]]")
    table.add_row("6", "â›” é€€å‡ºç³»ç»Ÿ", "[bold red]å®‰å…¨å…³é—­è¿æ¥[/]")

    console.print(table, justify="left")
    # åŠ¨æ€æç¤º
    # console.print(
    #     "[dim]â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„[/]\n"
    #     "[bold]å¿«æ·æ“ä½œ:[/] æ”¯æŒä½¿ç”¨å‘½ä»¤è¡Œ, [cyan]-h[/]å‚æ•°å¸®åŠ©"
    # )
    
# æ˜¯å¦å­˜åœ¨ks3ä¸­çš„è·¯å¾„åˆ†éš”ç¬¦â€œ/â€
def has_any_path_separator(path_str):
    return '/' in path_str[1:-1]  # æ£€æŸ¥ä¸­é—´æ˜¯å¦åŒ…å«`/`
    
class RobotDataUploader:
    def __init__(self, use_direct_auth=False):
        # æ•°é‡‡å¹³å°ç›¸å…³å‚æ•°
        self.eai_token = None
        self.eai_task_id = None
        self.eai_upload_task_id = None
        # ä¸Šä¼ å·¥å…·ç›¸å…³å‚æ•°
        self.sts_token = None
        self.connection = None
        self.resume_dir = ".upload_resume"
        self.file_filters = ["*.*"]  # "*.txt", "*.csv", "*.json", "*.dat" , "*.tar", "*.png" # é»˜è®¤æ–‡ä»¶è¿‡æ»¤å™¨
        self.use_direct_auth = use_direct_auth
        self.max_worker = 4
        
        # åˆ›å»ºæ–­ç‚¹ç»­ä¼ ç›®å½•
        if not os.path.exists(self.resume_dir):
            os.makedirs(self.resume_dir)
            
    
    def set_sts_token(self, sts_token):
        self.sts_token = sts_token
    
    def set_eai_token(self, eai_token):
        self.eai_token = eai_token
        
    def set_max_worker(self, max_worker):
        self.max_worker = max_worker
        
        
    def set_file_filters(self, filters):
        """è®¾ç½®æ–‡ä»¶è¿‡æ»¤å™¨"""
        self.file_filters = filters
    
    def _is_file_allowed(self, filename):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ç¬¦åˆè¿‡æ»¤è§„åˆ™"""
        import fnmatch
        # å¦‚æœè¿‡æ»¤å™¨åŒ…å«"*.*"ï¼Œè¡¨ç¤ºå…è®¸æ‰€æœ‰æ–‡ä»¶
        if "*.*" in self.file_filters:
            return True
        # å¦åˆ™æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä¸€è¿‡æ»¤è§„åˆ™
        return any(fnmatch.fnmatch(filename.lower(), pattern.lower()) 
                  for pattern in self.file_filters)
    
    def _get_file_md5(self, file_path):
        """è®¡ç®—æ–‡ä»¶MD5"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def _get_resume_info(self, file_path):
        """è·å–æ–­ç‚¹ç»­ä¼ ä¿¡æ¯"""
        resume_file = os.path.join(
            self.resume_dir, 
            f"{self._get_file_md5(file_path)}.json"
        )
        if os.path.exists(resume_file):
            with open(resume_file, 'r') as f:
                return json.load(f)
        return None
    
    def _save_resume_info(self, file_path, info):
        """ä¿å­˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯"""
        resume_file = os.path.join(
            self.resume_dir,
            f"{self._get_file_md5(file_path)}.json"
        )
        with open(resume_file, 'w') as f:
            json.dump(info, f)
    
    def _delete_resume_info(self, file_path):
        """åˆ é™¤æ–­ç‚¹ç»­ä¼ ä¿¡æ¯"""
        resume_file = os.path.join(
            self.resume_dir,
            f"{self._get_file_md5(file_path)}.json"
        )
        if os.path.exists(resume_file):
            os.remove(resume_file)
    
    def get_connection(self, show_progress=True):
        """è·å–è¿æ¥å¯¹è±¡ï¼Œæ ¹æ®é…ç½®ä½¿ç”¨STSæˆ–ç›´æ¥è®¤è¯"""
        if self.connection:
            return self.connection
        try:
            if self.use_direct_auth:
                if show_progress:
                    print(f"{Fore.BLUE}ä½¿ç”¨ç›´æ¥è®¤è¯æ–¹å¼...")
                self.connection = Connection(
                    access_key_id=config.ACCESS_KEY,
                    access_key_secret=config.SECRET_KEY,
                    host=config.ENDPOINT,
                    enable_crc=False
                )
            else:
                if show_progress:
                    print(f"{Fore.BLUE}ä½¿ç”¨STSä¸´æ—¶å‡­è¯è®¤è¯æ–¹å¼...")
                # response = requests.get(f"{config.SERVER_URL}/get_sts_token", verify=False)
                # self.sts_token = response.json()
                
                self.connection = Connection(
                    access_key_id=self.sts_token['accessKeyId'],
                    access_key_secret=self.sts_token['secretAccessKey'],
                    security_token=self.sts_token['securityToken'],
                    host=config.ENDPOINT,
                    enable_crc=False
                )
                
            return self.connection
        except Exception as e:
            print(f"{Fore.RED}è·å–è¿æ¥å¤±è´¥: {str(e)}")
            if self.use_direct_auth:
                print(f"{Fore.RED}è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„ ACCESS_KEY å’Œ SECRET_KEY æ˜¯å¦æ­£ç¡®")
            else:
                print(f"{Fore.RED}STSæœåŠ¡å¯èƒ½ä¸å¯ç”¨ï¼Œè¯·å°è¯•ä½¿ç”¨ç›´æ¥è®¤è¯æ–¹å¼")
            sys.exit(1)
            
    def get_ks3_sts(self):
        """è·å–é‡‘å±±äº‘ks3çš„stsä¿¡æ¯
        Args:
            token: å…·èº«æ•°æ®å¹³å°token
        Returns:
            _type_: æˆåŠŸ/å¤±è´¥
        """
        try:
            # è·å–STSæœåŠ¡ä¸Šä¼ å‡­è¯
            headers = {"Authorization": f"Bearer {self.eai_token}"}
            response = requests.get(f"{config.SERVER_URL}{config.STS_PATH}", headers=headers)
            if "code" in response.json() and response.json()["code"]==200:
                self.sts_token = response.json()["data"]
                return True
            return False
        except Exception as e:
            print(f"{Fore.RED}è·å–stså‡­è¯å¤±è´¥: {str(e)}")
            
                    
    def get_eai_token(self, ak, sk):
        """è·å–é‡‘å±±äº‘ks3çš„stsä¿¡æ¯
        Args:
            ak: å…·èº«æ•°æ®å¹³å°accesskey
            sk: å…·èº«æ•°æ®å¹³å°secretkey
        Returns:
            _type_: å…·èº«æ•°æ®å¹³å°token
        """
        try:
            # è·å–å…·èº«çœŸæœºå¹³å°token
            response = requests.post(f"{config.SERVER_URL}{config.TOKEN_PATH}", json={"ak":ak, "sk":sk})
            if response.status_code == 200:
                if "code" in response.json() and response.json()["code"]==200:
                     self.eai_token = response.json()["data"]['token']
                return self.eai_token
            return None
        except Exception as e:
            print(f"{Fore.RED}è·å–tokenå¤±è´¥: {str(e)}")
            
            
    def get_eai_task(self, task_id):
        """è·å–å…·èº«æ•°æ®å¹³å°ä»»åŠ¡è¯¦æƒ…
        Args:
            task_id: å…·èº«å¹³å°ä»»åŠ¡ID
            token: å…·èº«æ•°æ®å¹³å°token
        """
        try:
            if task_id == -99:
                self.eai_task_id = -99
                return True
            # è·å–STSæœåŠ¡ä¸Šä¼ å‡­è¯
            headers = {"Authorization": self.eai_token}
            response = requests.get(f"{config.SERVER_URL}{config.TASK_PATH}/{task_id}", headers=headers)
            if response.status_code == 200:
                if "code" in response.json() and response.json()["code"]==200:
                    self.eai_task_id = response.json()["data"]["id"]
                    return True
            return False
        except Exception as e:
            print(f"{Fore.RED}è·å–å…·èº«ä»»åŠ¡å¤±è´¥: {str(e)}")
            
        
    def beigin_upload_eai_task(self, data):
        """å¼€å§‹ä¸Šä¼ æ•°æ®é›†ä»»åŠ¡
        Args:
            data: æ•°æ®å®šä¹‰
        Returns:
            _type_: æˆåŠŸ/å¤±è´¥
        """
        try:
            headers = {"Authorization": self.eai_token}
            # è·å–å…·èº«çœŸæœºå¹³å°token
            response = requests.post(f"{config.SERVER_URL}{config.START_UPLOAD_PATH}", headers=headers, json=data)
            if response.status_code == 200:
                if "code" in response.json() and response.json()["code"]==200:
                    self.eai_upload_task_id = response.json()["data"]["uploadTaskId"]
                    return True
            print(f"{Fore.YELLOW}è­¦å‘Šï¼šå¼€å§‹ä¸Šä¼ é€šçŸ¥å¼‚å¸¸,æ•°æ®å¯æ­£å¸¸ä¸Šä¼ ,ä½†åç»­å¹³å°æ— è®°å½•,è¯·è”ç³»ç®¡ç†å‘˜æ’æŸ¥ã€‚å…·ä½“ä¿¡æ¯:{response.json()}")
            return False
        except Exception as e:
            print(f"{Fore.YELLOW}å¼€å§‹ä¸Šä¼ é€šçŸ¥å¼‚å¸¸: {str(e)}")
            
    
    def update_upload_eai_task_progress(self, data):
        """æ›´æ–°æ•°æ®é›†ä¸Šä¼ ä»»åŠ¡è¿›åº¦
        Args:
            data: æ•°æ®å®šä¹‰
        Returns:
            _type_: æˆåŠŸ/å¤±è´¥
        """
        try:
            headers = {"Authorization": self.eai_token}
            # è·å–å…·èº«çœŸæœºå¹³å°token
            response = requests.post(f"{config.SERVER_URL}{config.UPDATE_UPLOAD_PATH}", headers=headers, json=data)
            if response.status_code == 200:
                if "code" in response.json() and response.json()["code"]==200:
                    # self.eai_upload_task_id = response.json()["data"]["uploadTaskId"]
                    return True
            print(f"{Fore.YELLOW}è­¦å‘Šï¼šæ›´æ–°ä¸Šä¼ æ•°æ®é›†è¿›åº¦é€šçŸ¥å¼‚å¸¸,æ•°æ®å¯æ­£å¸¸ä¸Šä¼ ,ä½†å¹³å°è¿›åº¦å¼‚å¸¸,è¯·è”ç³»ç®¡ç†å‘˜æ’æŸ¥ã€‚å…·ä½“ä¿¡æ¯:{response.json()}")
            return False
        except Exception as e:
            print(f"{Fore.YELLOW}è­¦å‘Šï¼šæ›´æ–°ä¸Šä¼ æ•°æ®é›†è¿›åº¦é€šçŸ¥å¼‚å¸¸: {str(e)}")
            
            
    def complete_upload_eai_task(self, status):
        """å®Œæˆæ•°æ®é›†ä¸Šä¼ 
        Args:
            status: SUCCESS/FAILED
        Returns:
            _type_: æˆåŠŸ/å¤±è´¥
        """
        try:
            headers = {"Authorization": self.eai_token}
            # è·å–å…·èº«çœŸæœºå¹³å°token
            response = requests.post(f"{config.SERVER_URL}{config.COMPLETE_UPLOAD_PATH}", headers=headers, json={"upload_task_id":self.eai_upload_task_id, "status": status})
            if response.status_code == 200:
                if "code" in response.json() and response.json()["code"]==200:
                    # self.eai_upload_task_id = response.json()["data"]["uploadTaskId"]
                    return True
            print(f"{Fore.YELLOW}è­¦å‘Šï¼šå®Œæˆæ•°æ®é›†ä¸Šä¼ é€šçŸ¥å¼‚å¸¸,æ•°æ®å¯æ­£å¸¸ä¸Šä¼ ,ä½†å¹³å°çŠ¶æ€å¼‚å¸¸,è¯·è”ç³»ç®¡ç†å‘˜æ’æŸ¥ã€‚å…·ä½“ä¿¡æ¯:{response.json()}")
            return False
        except Exception as e:
            print(f"{Fore.YELLOW}å®Œæˆæ•°æ®é›†ä¸Šä¼ é€šçŸ¥å¼‚å¸¸: {str(e)}") 
            
    
    def upload_file(self, file_path, target_directory, base_dir=None, skip_dir_check=False, show_progress=True, pbar=None):
        """ä¸Šä¼ æ–‡ä»¶
        Args:
            file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            target_directory: è¿œç¨‹æ•°æ®é›†ç›®å½•
            base_dir: åŸºç¡€ç›®å½•è·¯å¾„ï¼ˆç”¨äºè®¡ç®—ç›¸å¯¹è·¯å¾„ï¼‰
            skip_dir_check: æ˜¯å¦è·³è¿‡ç›®å½•å­˜åœ¨æ£€æŸ¥
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†çš„ä¸Šä¼ è¿›åº¦(å•æ–‡ä»¶ä¸Šä¼ æ—¶ä¸ºTrue,å¯å±•ç¤ºè¿›åº¦æ¡ç­‰)
            pbar: å¤–éƒ¨ä¼ å…¥çš„è¿›åº¦æ¡å¯¹è±¡ï¼ˆç”¨äºæ‰¹é‡ä¸Šä¼ æ—¶å¤ç”¨ï¼‰
        """
        if not target_directory:
            print(f"{Fore.RED}é”™è¯¯ï¼šå¿…é¡»æŒ‡å®šæ•°æ®é›†åç§°")
            return
            
        if not os.path.exists(file_path):
            print(f"{Fore.RED}é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ - {file_path}")
            return
            
        if not os.path.isfile(file_path):
            print(f"{Fore.RED}é”™è¯¯ï¼šè·¯å¾„ä¸æ˜¯æ–‡ä»¶ - {file_path}")
            return
            
        if not self._is_file_allowed(os.path.basename(file_path)):
            print(f"{Fore.YELLOW}è·³è¿‡ä¸ç¬¦åˆè¿‡æ»¤è§„åˆ™çš„æ–‡ä»¶: {file_path}")
            return
            
            
        total_file_count, success_file_count, failed_file_count= 1, 0, 0
        if show_progress:
            # é€šçŸ¥å…·èº«æ•°æ®å¹³å°å¼€å§‹ä¸Šä¼ 
            data = {
                "eai_task_id": self.eai_task_id,    # ä»»åŠ¡id
                "task_name": target_directory,  # ä¸Šä¼ ä»»åŠ¡å¯¹åº”çš„æ•°æ®é›†
                "source_path": file_path,  # æºï¼šæ•°æ®é›†æºè·¯å¾„
                "target_root_path": target_directory,  # ç›®æ ‡ï¼šks3è¯¥æ•°æ®é›†æ ¹è·¯å¾„
                "target_full_path": target_directory,      # ç›®æ ‡ï¼šæ•°æ®é›†ks3å…¨è·¯å¾„
                "total_file_count": total_file_count,  # æ•°æ®é›†çš„æ€»æ–‡ä»¶æ•°é‡
                "total_size_bytes": os.path.getsize(file_path)  # æ•°æ®é›†å¤§å°ï¼ˆå­—èŠ‚ï¼‰
            }
            if not self.beigin_upload_eai_task(data=data):
                print(f"{Fore.YELLOW}è­¦å‘Šï¼šå¼€å§‹ä¸Šä¼ é€šçŸ¥å¼‚å¸¸,æ•°æ®å¯æ­£å¸¸ä¸Šä¼ ,ä½†åç»­å¹³å°æ— è®°å½•,è¯·è”ç³»ç®¡ç†å‘˜æ’æŸ¥ã€‚å…·ä½“ä¿¡æ¯:{data}")
            
        # æœ€å¤§é‡è¯•æ¬¡æ•°
        max_retries = 5
        retry_count = 0
        try:
            while retry_count < max_retries:
                try:
                    connection = self.get_connection(show_progress)
                    
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„
                    if base_dir:
                        # å°† base_dir å’Œ file_path éƒ½è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                        abs_base = os.path.abspath(base_dir)   # æœ¬åœ°æ•°æ®æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
                        abs_file = os.path.abspath(file_path)  # æœ¬åœ°æ•°æ®æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
                        
                        # ç¡®ä¿ file_path åœ¨ base_dir ä¸‹
                        if not abs_file.startswith(abs_base):
                            raise ValueError(f"æ–‡ä»¶è·¯å¾„ {file_path} ä¸åœ¨åŸºç¡€ç›®å½• {base_dir} ä¸‹")
                        
                        # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œå°†Windowsè·¯å¾„åˆ†éš”ç¬¦è½¬æ¢ä¸ºæ­£æ–œæ 
                        rel_path = os.path.relpath(abs_file, abs_base).replace('\\', '/').lstrip('/')
                        
                        # æ„å»ºç›®æ ‡é”®,é¿å…æ“ä½œç³»ç»Ÿè·¯å¾„é—®é¢˜
                        # key = os.path.join(config.UPLOAD_TARGET, sub_dir, rel_path)
                        key = f"{config.UPLOAD_TARGET}/{target_directory}/{rel_path}"
                    else:
                        # å¦‚æœæ²¡æœ‰æŒ‡å®š base_dirï¼Œåˆ™ç›´æ¥ä½¿ç”¨æ–‡ä»¶å
                        # key = os.path.join(config.UPLOAD_TARGET, sub_dir, os.path.basename(file_path))
                        key = f"{config.UPLOAD_TARGET}/{target_directory}/{os.path.basename(file_path)}"

                    # åªåœ¨å•æ–‡ä»¶ä¸Šä¼ ä¸”æœªæŒ‡å®šè·³è¿‡æ—¶æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
                    if not skip_dir_check:
                        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨
                        bucket = connection.get_bucket(config.BUCKET_NAME)
                        # prefix = os.path.join(config.UPLOAD_TARGET, sub_dir)
                        prefix = f"{config.UPLOAD_TARGET}/{target_directory}"
                        existing = list(bucket.list(prefix=prefix, delimiter='/', max_keys=1))
                        
                        if existing:
                            new_sub_dir = self._handle_duplicate_dataset(target_directory)
                            if not new_sub_dir:
                                return  # ç”¨æˆ·å–æ¶ˆä¸Šä¼ 
                            target_directory = new_sub_dir
                        # key = os.path.join(config.UPLOAD_TARGET, sub_dir, os.path.basename(file_path))
                        key = f"{config.UPLOAD_TARGET}/{target_directory}/{os.path.basename(file_path)}"
                    file_size = os.path.getsize(file_path)
                    
                    # å¤§å°æ–‡ä»¶çš„ä¸Šä¼ é€»è¾‘
                    if file_size > 5 * 1024 * 1024:  # 5MB
                        # self._multipart_upload(file_path, key, show_progress)
                        self._multipart_upload_ks3_sdk(file_path, key, show_progress, pbar=pbar)
                        # self._upload_large_file_with_progress(file_path, key, show_progress)
                    else:
                        self._simple_upload(file_path, key, show_progress, pbar=pbar)
                        
                    if show_progress and not pbar:
                        print(f"{Fore.GREEN}æˆåŠŸä¸Šä¼ : {file_path} åˆ° {key}")      
                    # ä¸Šä¼ æˆåŠŸï¼Œè·³å‡ºå¾ªç¯
                    success_file_count += 1
                    break
    
                except Exception as e:
                    retry_count += 1
                    # if show_progress:
                    if retry_count < max_retries:
                        print(f"{Fore.YELLOW}ä¸Šä¼ å¤±è´¥ {file_path}: {str(e)}ï¼Œæ­£åœ¨è¿›è¡Œç¬¬ {retry_count} æ¬¡é‡è¯•...")
                    else:
                        print(f"{Fore.RED}ä¸Šä¼ å¤±è´¥ {file_path}: {str(e)}ï¼Œå·²é‡è¯• {retry_count} æ¬¡ï¼Œæ”¾å¼ƒä¸Šä¼ ")
                    
                    if retry_count >= max_retries:
                        failed_file_count += 1
                        raise  # é‡è¯•æ¬¡æ•°ç”¨å®Œï¼ŒæŠ›å‡ºå¼‚å¸¸
                        # break
                    import time
                    # é‡è¯•å‰ç­‰å¾…ä¸€æ®µæ—¶é—´
                    time.sleep(2)
                
        finally:
            if show_progress:
                # æ›´æ–°è¿›åº¦æ•°æ®
                progress_data = {
                    "upload_task_id": self.eai_upload_task_id,   # ä¸Šä¼ æ•°æ®ä»»åŠ¡id
                    "success_file_count": success_file_count,   # å·²æˆåŠŸä¸Šä¼ æ–‡ä»¶æ•°é‡
                    "failed_file_count": failed_file_count       # ä¸Šä¼ å¤±è´¥æ–‡ä»¶æ•°é‡
                }
                # è°ƒç”¨è¿›åº¦æ›´æ–°æ¥å£
                self.update_upload_eai_task_progress(progress_data)
                # 3.é€šçŸ¥å…·èº«æ•°æ®å¹³å°å®Œæˆä¸Šä¼ 
                self.complete_upload_eai_task(status="SUCCESS")


    
    def _handle_duplicate_dataset(self, sub_dir, show_progress=True):
        """å¤„ç†é‡å¤çš„æ•°æ®é›†åç§°
        
        Returns:
            str or None: è¿”å›æ–°çš„æ•°æ®é›†åç§°ï¼Œå¦‚æœç”¨æˆ·å–æ¶ˆåˆ™è¿”å›None
        """
        while True:
            choice = input(f"{Fore.YELLOW}ç›®æ ‡æ•°æ®é›† '{sub_dir}' å·²å­˜åœ¨ã€‚è¯·é€‰æ‹©æ“ä½œ:\n"
                          f"1. ä½¿ç”¨å¦ä¸€ä¸ªæ•°æ®é›†åç§°\n"
                          f"2. ç»§ç»­ä½¿ç”¨æ­¤æ•°æ®é›†åç§°\n"
                          f"3. å–æ¶ˆä¸Šä¼ \n"
                          f"è¯·é€‰æ‹© [1/2/3]: ")
            
            if choice == '1':
                new_name = input(f"{Fore.BLUE}è¯·è¾“å…¥æ–°çš„ç›®æ ‡æ•°æ®é›†è·¯å¾„: ")
                if not new_name:
                    print(f"{Fore.RED}é”™è¯¯ï¼šç›®æ ‡æ•°æ®é›†è·¯å¾„ä¸èƒ½ä¸ºç©º")
                    continue
                if not has_any_path_separator(sub_dir):
                    print(f"{Fore.RED}é”™è¯¯: å¿…é¡»è‡³å°‘å«æœ‰1ä¸ªè·¯å¾„åˆ†éš”ç¬¦'/',å¦‚:a/b")
                    continue
                # é€’å½’æ£€æŸ¥æ–°åç§°æ˜¯å¦ä¹Ÿå­˜åœ¨
                bucket = self.get_connection(show_progress).get_bucket(config.BUCKET_NAME)
                prefix = f"{config.UPLOAD_TARGET}/{new_name}"
                existing = list(bucket.list(prefix=prefix, delimiter='/', max_keys=1))
                
                if existing:
                    return self._handle_duplicate_dataset(new_name, show_progress)
                return new_name
            elif choice == '2':
                return sub_dir
            elif choice == '3':
                return None
            else:
                print(f"{Fore.RED}æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    def _simple_upload(self, file_path, key, show_progress=True, pbar=None):
        """ç®€å•ä¸Šä¼ """
        file_size = os.path.getsize(file_path)
        
        # å¦‚æœæ²¡æœ‰ä¼ å…¥å¤–éƒ¨è¿›åº¦æ¡ä¸”éœ€è¦æ˜¾ç¤ºè¿›åº¦ï¼Œåˆ™åˆ›å»ºæ–°çš„è¿›åº¦æ¡
        local_pbar = None
        current_pbar = pbar
        
        if show_progress and pbar is None:
            local_pbar = tqdm(total=file_size,
                        bar_format = "{l_bar}{bar:40}| {percentage:.0f}% [{elapsed}<{remaining}, {rate_fmt}{postfix}]",
                        colour = "GREEN" , # ä½¿ç”¨æ ‡å‡†ç»¿è‰²è€Œéåå…­è¿›åˆ¶é¢œè‰²ç 
                        dynamic_ncols = True , # è‡ªåŠ¨é€‚åº”ç»ˆç«¯å®½åº¦
                        unit='B', 
                        unit_scale=True, 
                        desc=os.path.basename(file_path))
            current_pbar = local_pbar
        
        # ç®€å•ä¸Šä¼ æ— æ³•è·å–å®æ—¶è¿›åº¦å›è°ƒï¼Œåªèƒ½åœ¨å¼€å§‹æˆ–ç»“æŸæ—¶æ›´æ–°
        # æˆ–è€…æ¨¡æ‹Ÿè¿›åº¦ï¼ˆä¸æ¨èï¼‰
        # è€ƒè™‘åˆ° KS3 SDK çš„ set_contents_from_file æ˜¯é˜»å¡çš„ä¸”æ²¡æœ‰å›è°ƒå‚æ•°
        # æˆ‘ä»¬åªèƒ½åœ¨ä¸Šä¼ å®Œæˆåä¸€æ¬¡æ€§æ›´æ–°è¿›åº¦
        
        with open(file_path, 'rb') as f:
            bucket = self.get_connection(show_progress).get_bucket(config.BUCKET_NAME)
            k = bucket.new_key(key)
            
            k.set_contents_from_file(f)
            
            # ä¸Šä¼ å®Œæˆåæ›´æ–°è¿›åº¦
            if current_pbar:
                current_pbar.update(file_size)
        
        if local_pbar:
            local_pbar.close()
    
    def _multipart_upload(self, file_path, key, show_progress=True):
        """åˆ†ç‰‡ä¸Šä¼ """        
        chunk_size = 5 * 1024 * 1024  # 5MBåˆ†ç‰‡
        file_size = os.path.getsize(file_path)
        
        bucket = self.connection.get_bucket(config.BUCKET_NAME)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„ä¸Šä¼ 
        resume_info = self._get_resume_info(file_path)
        if resume_info and resume_info['key'] == key:
            # æ¢å¤æœªå®Œæˆçš„ä¸Šä¼ 
            mp = bucket.get_all_multipart_uploads(prefix=key)[0]
            upload_id = resume_info['upload_id']
            completed_parts = resume_info['completed_parts']
            
            # todo æ¢å¤mpçš„part_crc_infosçŠ¶æ€(mpå¯¹è±¡ä¸Šä¼ æ—¶ä¼šåœ¨æœ¬åœ°å†…å­˜ç»´æŠ¤part_infoï¼Œç”¨äºå®Œæˆä¸Šä¼ æ—¶æ ¡éªŒæ•´ä¸ªæ–‡ä»¶çš„crcã€‚part_infoä¸å®Œæ•´ä¼šå¯¼è‡´æœ¬åœ°è®¡ç®—crcå‡ºé”™ã€‚)
            for part in completed_parts:
                # from ks3.multipart import PartInfo
                mp.part_crc_infos[part['PartNumber']] = PartInfo(part['PartSize'], part['Crc64ecma'])
            
            start_part = len(completed_parts) + 1
            # if show_progress:
            print(f"{Fore.GREEN}å‘ç°{key}çš„æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ï¼Œä»ç¬¬ {start_part} éƒ¨åˆ†ç»§ç»­ä¸Šä¼ ")
        else:
            # åˆå§‹åŒ–æ–°çš„åˆ†ç‰‡ä¸Šä¼ 
            mp = bucket.initiate_multipart_upload(key)
            upload_id = mp.id
            completed_parts = []
            start_part = 1
            
        # è®¡ç®—åˆ†ç‰‡æ•°é‡
        chunk_count = int(math.ceil(file_size * 1.0 / chunk_size))
        
        if show_progress:
            pbar = tqdm(total=file_size, 
                        bar_format = "{l_bar}{bar:40}| {percentage:.0f}% [{elapsed}<{remaining}, {rate_fmt}{postfix}]",
                        colour = "GREEN" , # ä½¿ç”¨æ ‡å‡†ç»¿è‰²è€Œéåå…­è¿›åˆ¶é¢œè‰²ç  
                        dynamic_ncols = True , # è‡ªåŠ¨é€‚åº”ç»ˆç«¯å®½åº¦
                        unit='B', 
                        unit_scale=True, 
                        desc=os.path.basename(file_path))
            completed_bytes = (start_part - 1) * chunk_size
            pbar.update(completed_bytes)
        
        try:
            for i in range(start_part - 1, chunk_count):
                offset = chunk_size * i
                bytes_to_read = min(chunk_size, file_size - offset)
                
                with FileChunkIO(file_path, 'r', offset=offset, bytes=bytes_to_read) as fp:
                    # ä¸Šä¼ åˆ†ç‰‡                   
                    part_num = i + 1
                    ret = mp.upload_part_from_file(fp, part_num=part_num)
                    
                    if show_progress:
                        pbar.update(bytes_to_read)
                    
                    completed_parts.append({
                        'PartNumber': part_num,
                        # todo é¢å¤–ä¿å­˜å—å¤§å°ã€å—crc64ä¿¡æ¯
                        'PartSize': bytes_to_read,
                        'ETag': ret.response_metadata.headers['ETag'],  # todo æ”¹ä¸ºäº†ä¿å­˜ETagä¿¡æ¯
                        'Crc64ecma': ret.response_metadata.headers['x-kss-checksum-crc64ecma']
                    })
                    
                    # ä¿å­˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
                    self._save_resume_info(file_path, {
                        'key': key,
                        'upload_id': upload_id,
                        'completed_parts': completed_parts
                    })
            
            mp.complete_upload()
            self._delete_resume_info(file_path)
         
        except Exception as e:
            if show_progress:
                print(f"{Fore.RED}ä¸Šä¼ å¤±è´¥ {file_path}: {str(e)}")
            raise
        finally:
            if show_progress:
                pbar.close()
    
    def _multipart_upload_ks3_sdk(self, file_path, key, show_progress=True, pbar=None):
        """
        ä½¿ç”¨é‡‘å±±äº‘SDKåŸç”Ÿæ–¹å¼å®ç°çš„åˆ†ç‰‡ä¸Šä¼ ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
        æ›¿ä»£åŸæœ‰çš„ _multipart_upload æ–¹æ³•
        å‚è€ƒæ–‡æ¡£: https://docs.ksyun.com/documents/40532?type=3
        """
        # åˆ†ç‰‡å¤§å° 5MB
        chunk_size = 5 * 1024 * 1024
        file_size = os.path.getsize(file_path)
        # è®¡ç®—åˆ†ç‰‡æ•°é‡
        chunk_count = int(math.ceil(file_size * 1.0 / chunk_size))
        
        bucket = self.connection.get_bucket(config.BUCKET_NAME)
        
        mp = None
        uploaded_parts_map = {} # part_number -> Part
        
        # 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœªå®Œæˆçš„åˆ†ç‰‡ä¸Šä¼ ä»»åŠ¡ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        # è·å–è¯¥keyä¸‹çš„æ‰€æœ‰åˆ†ç‰‡ä¸Šä¼ ä»»åŠ¡
        # æ³¨æ„ï¼šget_all_multipart_uploads è¿”å›çš„æ˜¯Bucketä¸‹çš„æ‰€æœ‰åˆ†ç‰‡ä»»åŠ¡ï¼Œéœ€è¦è¿‡æ»¤
        all_uploads = bucket.get_all_multipart_uploads(prefix=key)
        for upload in all_uploads:
            if upload.key_name == key:
                mp = upload
                break
        
        if mp:
            # if show_progress and pbar is None:
            print(f"{Fore.GREEN}å‘ç°æœªå®Œæˆçš„ä¸Šä¼ ä»»åŠ¡ (ID: {mp.id})ï¼Œæ­£åœ¨è·å–å·²ä¸Šä¼ åˆ†ç‰‡ä¿¡æ¯...")
            
            # è·å–å·²ä¸Šä¼ çš„åˆ†ç‰‡
            # ä½¿ç”¨SDKæä¾›çš„è¿­ä»£å™¨è‡ªåŠ¨å¤„ç†åˆ†é¡µï¼Œè·å–æ‰€æœ‰å·²ä¸Šä¼ åˆ†ç‰‡
            for part in mp:
                uploaded_parts_map[part.part_number] = part
                # å°è¯•ä» part å¯¹è±¡ä¸­æ¢å¤ CRC64 ä¿¡æ¯
                # æ³¨æ„ï¼šè¿™ä¾èµ–äº KS3 SDK åœ¨ ListParts æ—¶æ˜¯å¦è¿”å›å¹¶è§£æäº† Crc64ecma
                # å¦‚æœ part å¯¹è±¡æœ‰è¯¥å±æ€§ï¼ˆé€šå¸¸æ˜¯æ‰©å±•å±æ€§æˆ–é€šè¿‡ xml è§£æå¾—åˆ°ï¼‰ï¼Œåˆ™èµ‹å€¼
                # å¦‚æœæ²¡æœ‰ï¼Œåç»­ complete_upload ä»å¯èƒ½æŠ¥é”™ï¼Œéœ€è¦ catch å¤„ç†
                # if hasattr(part, 'crc64ecma') and part.crc64ecma:
                #      mp.part_crc_infos[part.part_number] = PartInfo(part.size, part.crc64ecma)
                # elif hasattr(part, 'last_modified'): # åªè¦æ˜¯æœ‰æ•ˆçš„part
                #      # å¦‚æœæ²¡æœ‰ CRC ä¿¡æ¯ï¼Œæˆ‘ä»¬åœ¨æœåŠ¡ç«¯æ¨¡å¼ä¸‹ä¹Ÿæ— æ³•å‡­ç©ºæé€ ã€‚
                #      # ä½†ä¸ºäº†å…¼å®¹ _multipart_upload çš„é€»è¾‘ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œå°è¯•æŸ¥æ‰¾æœ¬åœ°æ˜¯å¦ç¢°å·§æœ‰è®°å½•
                #      # (è™½ç„¶ _multipart_upload_ks3_sdk æ—¨åœ¨æ‘†è„±æœ¬åœ°è®°å½•ä¾èµ–)
                     
                #      # å°è¯•ä»æœ¬åœ°æ–­ç‚¹ç»­ä¼ æ–‡ä»¶è¡¥å…… CRC ä¿¡æ¯ (æ··åˆæ¨¡å¼)
                #      resume_info = self._get_resume_info(file_path)
                #      if resume_info and resume_info.get('upload_id') == mp.id:
                #          for p in resume_info.get('completed_parts', []):
                #              if p['PartNumber'] == part.part_number:
                #                  mp.part_crc_infos[part.part_number] = PartInfo(p['PartSize'], p['Crc64ecma'])
                #                  break

            # if show_progress and pbar is None:
            print(f"{Fore.GREEN}å·²å®Œæˆåˆ†ç‰‡: {len(uploaded_parts_map)}/{chunk_count}")
        else:
            # åˆå§‹åŒ–æ–°çš„åˆ†ç‰‡ä¸Šä¼ ä»»åŠ¡
            # x-kss-storage-class: STANDARD (æ ‡å‡†) / STANDARD_IA (ä½é¢‘)
            headers = {"x-kss-storage-class": "STANDARD"}
            mp = bucket.initiate_multipart_upload(key, headers=headers)
            if show_progress and pbar is None:
                print(f"{Fore.GREEN}åˆå§‹åŒ–æ–°çš„ä¸Šä¼ ä»»åŠ¡ (ID: {mp.id})")

        # 2. å‡†å¤‡è¿›åº¦æ¡
        local_pbar = None
        current_pbar = pbar
        # è®¡ç®—å·²ä¸Šä¼ çš„å­—èŠ‚æ•°
        completed_bytes = 0
        for part in uploaded_parts_map.values():
            completed_bytes += part.size
            
        if show_progress and pbar is None:
            local_pbar = tqdm(total=file_size,
                        initial=completed_bytes,
                        bar_format="{l_bar}{bar:40}| {percentage:.0f}% [{elapsed}<{remaining}, {rate_fmt}{postfix}]",
                        colour="GREEN",
                        dynamic_ncols=True,
                        unit='B',
                        unit_scale=True,
                        desc=os.path.basename(file_path))
            current_pbar = local_pbar
        # æ›´æ–°è¿›åº¦æ¡
        current_pbar.update(completed_bytes)
        try:
            # 3. ä¸Šä¼ æœªå®Œæˆçš„åˆ†ç‰‡
            # è®°å½•æœ¬æ¬¡ä¸Šä¼ çš„åˆ†ç‰‡ä¿¡æ¯ï¼Œä»¥ä¾¿æ›´æ–°æœ¬åœ°ç¼“å­˜ï¼ˆå¯é€‰ï¼Œç”¨äºè¾…åŠ©ä¸‹ä¸€æ¬¡ CRC æ¢å¤ï¼‰
            current_session_parts = []
            
            for i in range(chunk_count):
                part_number = i + 1
                
                # å¦‚æœè¯¥åˆ†ç‰‡å·²ä¸Šä¼ ï¼Œåˆ™è·³è¿‡
                if part_number in uploaded_parts_map:
                    continue
                
                offset = i * chunk_size
                bytes_to_read = min(chunk_size, file_size - offset)
                
                # ä½¿ç”¨FileChunkIOè¯»å–æŒ‡å®šåˆ†ç‰‡çš„æ•°æ®
                with FileChunkIO(file_path, 'r', offset=offset, bytes=bytes_to_read) as fp:
                    # ä¸Šä¼ åˆ†ç‰‡
                    ret = mp.upload_part_from_file(fp, part_num=part_number)
                    
                    # æ”¶é›†æœ¬æ¬¡ä¸Šä¼ çš„åˆ†ç‰‡ä¿¡æ¯ï¼Œç”¨äºæ›´æ–°æœ¬åœ°ç¼“å­˜
                    # if ret and hasattr(ret, 'response_metadata'):
                    #     crc64 = ret.response_metadata.headers.get('x-kss-checksum-crc64ecma')
                    #     etag =ret.response_metadata.headers.get('ETag')
                    #     if crc64:
                    #          part_info = {
                    #             'PartNumber': part_number,
                    #             'PartSize': bytes_to_read,
                    #             'Crc64ecma': crc64,
                    #             'ETag': etag
                    #         }
                    #          current_session_parts.append(part_info)
                    #          # åŒæ—¶æ›´æ–° mp å¯¹è±¡çš„å†…å­˜çŠ¶æ€ï¼Œå¢åŠ æˆåŠŸçš„å‡ ç‡
                    #          mp.part_crc_infos[part_number] = PartInfo(bytes_to_read, crc64)
                             
                    #          # å®æ—¶æ›´æ–°æœ¬åœ°æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ï¼Œæ¯ä¸Šä¼ ä¸€ä¸ªåˆ†ç‰‡å°±ä¿å­˜ä¸€æ¬¡
                    #          # è™½ç„¶å¢åŠ äº†IOå¼€é”€ï¼Œä½†ä¿è¯äº†æ–­ç‚¹ç»­ä¼ çš„å®æ—¶æ€§
                    #          resume_info = self._get_resume_info(file_path) or {
                    #              'key': key,
                    #              'upload_id': mp.id,
                    #              'completed_parts': []
                    #          }
                             
                    #          # ç¡®ä¿ upload_id åŒ¹é…ï¼Œå¦‚æœä¸åŒ¹é…è¯´æ˜æ˜¯æ–°çš„ä»»åŠ¡ï¼Œé‡ç½® completed_parts
                    #          if resume_info.get('upload_id') != mp.id:
                    #              resume_info = {'key': key, 'upload_id': mp.id, 'completed_parts': []}
                             
                    #          # å°†æ–°ä¸Šä¼ çš„åˆ†ç‰‡æ·»åŠ åˆ°è®°å½•ä¸­
                    #          # ç®€å•èµ·è§ï¼Œç›´æ¥è¿½åŠ ï¼Œä¿å­˜æ—¶å»é‡åœ¨ _save_resume_info ä¸­é€šå¸¸ä¸åšï¼Œ
                    #          # ä½†æˆ‘ä»¬ä¸ºäº†ä¿æŒæ–‡ä»¶æ•´æ´ï¼Œå¯ä»¥è¯»å–-æ›´æ–°-ä¿å­˜
                             
                    #          # æ•ˆç‡ä¼˜åŒ–ï¼šä¸éœ€è¦æ¯æ¬¡éƒ½é‡æ–°è¯»å–æ•´ä¸ªåˆ—è¡¨å†å»é‡
                    #          # åªéœ€è¦å°†å½“å‰ part_info è¿½åŠ åˆ° completed_parts å¹¶ä¿å­˜
                    #          resume_info['completed_parts'].append(part_info)
                    #          self._save_resume_info(file_path, resume_info)

                
                if current_pbar:
                    current_pbar.update(bytes_to_read)

            
            # 4. å®Œæˆåˆ†ç‰‡ä¸Šä¼ 
            # SDKä¼šè‡ªåŠ¨åˆå¹¶åˆ†ç‰‡
            try:
                mp.complete_upload()
                # æˆåŠŸåæ¸…ç†æœ¬åœ°ç¼“å­˜
                # self._delete_resume_info(file_path)
            except Exception as e:
                # å¤„ç†æ–­ç‚¹ç»­ä¼ æ—¶çš„CRCä¸ä¸€è‡´é—®é¢˜
                # ç”±äºæ— æ³•è·å–æ—§åˆ†ç‰‡çš„CRCä¿¡æ¯ï¼Œä¼šå¯¼è‡´æœ¬åœ°è®¡ç®—CRCä¸æœåŠ¡ç«¯ä¸ä¸€è‡´
                # ä½†æ­¤æ—¶æœåŠ¡ç«¯å®é™…ä¸Šå·²ç»åˆå¹¶æˆåŠŸï¼ˆå¦åˆ™ä¸ä¼šè¿”å›server_crcï¼‰
                if "Inconsistent CRC checksum" in str(e):
                    if show_progress and pbar is None:
                        print(f"{Fore.YELLOW}è­¦å‘Š: CRCæ ¡éªŒä¸ä¸€è‡´(æ–­ç‚¹ç»­ä¼ å¯¼è‡´)ï¼Œä½†ä¸Šä¼ å·²å®Œæˆã€‚")
                    # æˆåŠŸåæ¸…ç†æœ¬åœ°ç¼“å­˜
                    # self._delete_resume_info(file_path)
                else:
                    raise e
            
            if local_pbar:
                local_pbar.close()
                # print(f"{Fore.GREEN}ä¸Šä¼ æˆåŠŸ: {key}")
                
        except Exception as e:
            if local_pbar:
                local_pbar.close()
            if show_progress and pbar is None:
                print(f"{Fore.RED}ä¸Šä¼ å¤±è´¥ {file_path}: {str(e)}")
            # æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚é‡è¯•é€»è¾‘æ•è·
            raise e

    def _upload_large_file_with_progress(self, file_path, key, show_progress=True):
        """
        ä½¿ç”¨ key.upload_file æ–¹æ³•è¿›è¡Œå¤§æ–‡ä»¶åˆ†ç‰‡ä¸Šä¼ ï¼Œé€šè¿‡å›è°ƒå‡½æ•°å±•ç¤ºè¿›åº¦
        
        Args:
            file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            key: ç›®æ ‡å­˜å‚¨é”®
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        """
        file_size = os.path.getsize(file_path)
        
        # å‡†å¤‡è¿›åº¦æ¡
        if show_progress:
            pbar = tqdm(total=file_size,
                        bar_format="{l_bar}{bar:40}| {percentage:.0f}% [{elapsed}<{remaining}, {rate_fmt}{postfix}]",
                        colour="GREEN",
                        dynamic_ncols=True,
                        unit='B',
                        unit_scale=True,
                        desc=os.path.basename(file_path))

            try:
                bucket = self.connection.get_bucket(config.BUCKET_NAME)
                k = bucket.new_key(key)
                
                k.upload_file(
                    filename=file_path,
                    part_size=5 * 1024 * 1024,  # 5MB åˆ†ç‰‡
                    threads_num=5,              # 5çº¿ç¨‹å¹¶å‘
                    resumable=True,             # å¼€å¯æ–­ç‚¹ç»­ä¼ 
                    resumable_filename=os.path.join(self.resume_dir, f"{self._get_file_md5(file_path)}.ks3resume"),
                    headers={'x-kss-storage-class': 'STANDARD'} # æ ‡å‡†å­˜å‚¨
                )
                
            except Exception as e:
                print(f"{Fore.RED}ä¸Šä¼ å¤±è´¥ {file_path}: {str(e)}")
                raise e
            finally:
                pbar.close()
                
        else:
            # ä¸æ˜¾ç¤ºè¿›åº¦æ¡çš„ä¸Šä¼ 
            try:
                bucket = self.connection.get_bucket(config.BUCKET_NAME)
                k = bucket.new_key(key)
                
                k.upload_file(
                    filename=file_path,
                    part_size=5 * 1024 * 1024,
                    threads_num=5,
                    resumable=True,
                    resumable_filename=os.path.join(self.resume_dir, f"{self._get_file_md5(file_path)}.ks3resume"),
                    headers={'x-kss-storage-class': 'STANDARD'}
                )
            except Exception as e:
                raise e

    
    def batch_upload(self, directory, target_directory):
        """æ‰¹é‡ä¸Šä¼ ç›®å½•ä¸‹çš„æ–‡ä»¶
        
        Args:
            directory: æœ¬åœ°æ•°æ®é›†ç›®å½•è·¯å¾„
            target_directory: è¿œç¨‹æ•°æ®é›†ç›®å½•è·¯å¾„
        """
        if not target_directory:
            print(f"{Fore.RED}é”™è¯¯ï¼šå¿…é¡»æŒ‡å®šæ•°æ®é›†è·¯å¾„")
            return
            
        if not os.path.exists(directory):
            print(f"{Fore.RED}é”™è¯¯ï¼šè·¯å¾„ç›®å½•ä¸å­˜åœ¨ - {directory}")
            return
            
        if not os.path.isdir(directory):
            print(f"{Fore.RED}é”™è¯¯ï¼šè·¯å¾„ä¸æ˜¯ç›®å½• - {directory}")
            return
            
        connection = self.get_connection()
        
        # åœ¨æ‰¹é‡ä¸Šä¼ å¼€å§‹å‰æ£€æŸ¥ä¸€æ¬¡ç›®å½•æ˜¯å¦å­˜åœ¨
        bucket = connection.get_bucket(config.BUCKET_NAME)
        # prefix = os.path.join(config.UPLOAD_TARGET, sub_dir)
        prefix = f"{config.UPLOAD_TARGET}/{target_directory}"

        existing = list(bucket.list(prefix=prefix, delimiter='/', max_keys=1))
        
        if existing:
            new_sub_dir = self._handle_duplicate_dataset(target_directory)
            if not new_sub_dir:
                return  # ç”¨æˆ·å–æ¶ˆä¸Šä¼ 
            target_directory = new_sub_dir
            
        # æ”¶é›†ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶åŠå…¶å¤§å°
        files_info = []
        total_size = 0
        for root, _, filenames in os.walk(directory):
            for filename in filenames:
                if self._is_file_allowed(filename):
                    file_path = os.path.join(root, filename)
                    size = os.path.getsize(file_path)
                    files_info.append((file_path, size))
                    total_size += size
                else:
                    print(f"{Fore.YELLOW}è·³è¿‡ä¸ç¬¦åˆè¿‡æ»¤è§„åˆ™çš„æ–‡ä»¶: {filename}")
        
        if not files_info:
            print(f"{Fore.YELLOW}è­¦å‘Šï¼šåœ¨ç›®å½• {directory} ä¸­æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆè¿‡æ»¤è§„åˆ™çš„æ–‡ä»¶")
            return
            
        # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼Œä¾¿äºå‡åŒ€åˆ†é…
        files_info.sort(key=lambda x: x[1], reverse=True)
        
        # å°†æ–‡ä»¶åˆ†é…ç»™ä¸åŒçº¿ç¨‹ï¼Œå°½é‡ä¿è¯æ¯ä¸ªçº¿ç¨‹å¤„ç†çš„æ•°æ®é‡æ¥è¿‘
        max_workers = min(self.max_worker, len(files_info)) 
        thread_files = [[] for _ in range(max_workers)]   
        thread_sizes = [0] * max_workers
        
        # ä½¿ç”¨è´ªå¿ƒç®—æ³•åˆ†é…æ–‡ä»¶
        for file_path, size in files_info:
            # æ‰¾åˆ°å½“å‰æ€»å¤§å°æœ€å°çš„çº¿ç¨‹
            min_size_thread = min(range(max_workers), key=lambda i: thread_sizes[i])
            thread_files[min_size_thread].append(file_path)
            thread_sizes[min_size_thread] += size
        
        print(f"{Fore.BLUE}æ‰¾åˆ° {len(files_info)} ä¸ªæ–‡ä»¶ (æ€»å¤§å°: {total_size/1024/1024:.2f}MB) å‡†å¤‡ä¸Šä¼ ...")
        # 1.é€šçŸ¥å…·èº«æ•°æ®å¹³å°å¼€å§‹ä¸Šä¼ 
        data = {
            "eai_task_id": self.eai_task_id,    # ä»»åŠ¡id
            "task_name": target_directory,  # ä¸Šä¼ ä»»åŠ¡å¯¹åº”çš„æ•°æ®é›†
            "source_path": directory,  # æºï¼šæ•°æ®é›†æºè·¯å¾„
            "target_root_path": target_directory,  # ç›®æ ‡ï¼šks3è¯¥æ•°æ®é›†æ ¹è·¯å¾„
            "target_full_path": target_directory,      # ç›®æ ‡ï¼šæ•°æ®é›†ks3å…¨è·¯å¾„
            "total_file_count": len(files_info),  # æ•°æ®é›†çš„æ€»æ–‡ä»¶æ•°é‡
            "total_size_bytes": total_size  # æ•°æ®é›†å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        }
        self.beigin_upload_eai_task(data=data)
        
        # å®šä¹‰çº¿ç¨‹ä¸Šä¼ ä»»åŠ¡
        def upload_task_thread(thread_id, files, total_size, shared_success_files, shared_failed_files, lock):
            # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„ä¸Šä¼ å™¨å®ä¾‹
            thread_uploader = RobotDataUploader(use_direct_auth=self.use_direct_auth)
            thread_uploader.set_sts_token(self.sts_token)
            # ä½¿ç”¨çº¿ç¨‹æœ¬åœ°å˜é‡è·Ÿè¸ªå½“å‰çº¿ç¨‹çš„æˆåŠŸå’Œå¤±è´¥æ–‡ä»¶
            local_success_files = []
            local_failed_files = []
            
            # æ·»åŠ è¿›åº¦æ›´æ–°è®¡æ—¶å™¨
            last_update_time = time.time()
            
            with tqdm(total=total_size, 
                     unit='B', 
                     colour = "GREEN" , # ä½¿ç”¨æ ‡å‡†ç»¿è‰²è€Œéåå…­è¿›åˆ¶é¢œè‰²ç 
                     dynamic_ncols = True , # è‡ªåŠ¨é€‚åº”ç»ˆç«¯å®½åº¦
                     unit_scale=True, 
                     desc=f"ğŸŸ¢ çº¿ç¨‹-{thread_id}", leave=True,
                     position=thread_id) as pbar:                
                for file_path in files:
                    try:
                        # ä¼ å…¥åŸºç¡€ç›®å½•è·¯å¾„
                        thread_uploader.upload_file(
                            file_path, 
                            target_directory,
                            base_dir=directory,  # æ·»åŠ åŸºç¡€ç›®å½•å‚æ•°
                            skip_dir_check=True, 
                            show_progress=False, # æ‰¹é‡ä¸Šä¼ æ—¶ï¼Œå†…éƒ¨ä¸å†æ‰“å°å•ä¸ªæ–‡ä»¶çš„è¿›åº¦æ¡ä¿¡æ¯ï¼Œä½†ä¼šé€šè¿‡pbaræ›´æ–°è¿›åº¦
                            pbar=pbar # ä¼ å…¥å½“å‰çº¿ç¨‹çš„è¿›åº¦æ¡å¯¹è±¡
                        )
                        local_success_files.append(file_path)
                        # æ›´æ–°å…±äº«æˆåŠŸåˆ—è¡¨
                        with lock:
                            shared_success_files.append(file_path)
                        
                        # æ›´æ–°è¿›åº¦æ¡
                        # æ³¨æ„ï¼šupload_fileå†…éƒ¨å·²ç»è°ƒç”¨äº†pbar.updateæ¥æ›´æ–°å®é™…ä¸Šä¼ çš„å­—èŠ‚æ•°
                        # è¿™é‡Œä¸éœ€è¦å†æ¬¡ update file_sizeï¼Œå¦åˆ™ä¼šå¯¼è‡´è¿›åº¦æ¡ç¿»å€æˆ–æº¢å‡º
                        # pbar.update(file_size) 
                    except Exception as e:
                        print(f"{Fore.RED}ä¸Šä¼ å¤±è´¥ {file_path}: {str(e)}")
                        local_failed_files.append(file_path)
                        # æ›´æ–°å…±äº«å¤±è´¥åˆ—è¡¨
                        with lock:
                            shared_failed_files.append(file_path)
                            
                        # å¤±è´¥çš„æƒ…å†µä¸‹ï¼Œupload_fileå†…éƒ¨å¯èƒ½æ²¡æœ‰å®Œå…¨updateè¿›åº¦
                        # ä¸ºäº†ä¿è¯è¿›åº¦æ¡èƒ½èµ°åˆ°ç»ˆç‚¹ï¼ˆæˆ–è€…è‡³å°‘åæ˜ è¯¥æ–‡ä»¶å·²å¤„ç†å®Œæ¯•ï¼‰ï¼Œ
                        # å¯ä»¥é€‰æ‹©åœ¨è¿™é‡Œè¡¥é½å‰©ä½™çš„è¿›åº¦ï¼Œæˆ–è€…ä»…ä»…è®°å½•å¤±è´¥ã€‚
                        # é€šå¸¸åšæ³•æ˜¯ï¼Œå¦‚æœä»»åŠ¡å¤±è´¥äº†ï¼Œè¿›åº¦æ¡å¯èƒ½ä¸ä¼šæ»¡ï¼Œè¿™èƒ½åæ˜ å‡ºé—®é¢˜ã€‚
                        # ä½†ä¸ºäº†è®©æ€»è¿›åº¦æ¡çœ‹èµ·æ¥"å®Œæˆ"äº†å½“å‰ä»»åŠ¡çš„å¤„ç†ï¼Œå¯ä»¥æ‰‹åŠ¨è¡¥é½è¯¥æ–‡ä»¶çš„å¤§å°ã€‚
                        # ä¸è¿‡éœ€è¦è®¡ç®—å·²ä¸Šä¼ çš„éƒ¨åˆ†ï¼Œæ¯”è¾ƒå¤æ‚ã€‚
                        # ç®€å•èµ·è§ï¼Œå¦‚æœæ˜¯å¤±è´¥ï¼Œæˆ‘ä»¬å‡è®¾è¯¥æ–‡ä»¶çš„è¿›åº¦æ²¡æœ‰å®Œæˆã€‚
                        # æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©å¼ºåˆ¶æ›´æ–°è¯¥æ–‡ä»¶çš„è¿›åº¦ï¼Œä»¥é¿å…è¿›åº¦æ¡å¡ä½ã€‚
                        # file_size = os.path.getsize(file_path)
                        # pbar.update(file_size)
                
                return local_success_files, local_failed_files
        
        # å®šä¹‰è¿›åº¦æ›´æ–°çº¿ç¨‹å‡½æ•°
        def update_progress_thread(shared_success_files, shared_failed_files, lock, stop_event):
            # ä½¿ç”¨å½“å‰å®ä¾‹è€Œä¸æ˜¯åˆ›å»ºæ–°çš„å®ä¾‹
            while not stop_event.is_set():
                # è·å–å½“å‰å…±äº«åˆ—è¡¨çš„é•¿åº¦
                with lock:
                    success_count = len(shared_success_files)
                    failed_count = len(shared_failed_files)
                
                # æ›´æ–°è¿›åº¦æ•°æ®
                progress_data = {
                    "upload_task_id": self.eai_upload_task_id,   # ä¸Šä¼ æ•°æ®ä»»åŠ¡id
                    "success_file_count": success_count,   # å·²æˆåŠŸä¸Šä¼ æ–‡ä»¶æ•°é‡
                    "failed_file_count": failed_count       # ä¸Šä¼ å¤±è´¥æ–‡ä»¶æ•°é‡
                }
                
                # è°ƒç”¨è¿›åº¦æ›´æ–°æ¥å£
                self.update_upload_eai_task_progress(progress_data)
                # print("haha")
                
                # æ¯5ç§’æ›´æ–°ä¸€æ¬¡
                time.sleep(5)
            
            # ä¸Šä¼ å®Œæˆåï¼Œå‘é€æœ€ç»ˆè¿›åº¦æ›´æ–°
            with lock:
                success_count = len(shared_success_files)
                failed_count = len(shared_failed_files)
            
            final_progress_data = {
                "upload_task_id": self.eai_upload_task_id,   # ä¸Šä¼ æ•°æ®ä»»åŠ¡id
                "success_file_count": success_count,   # å·²æˆåŠŸä¸Šä¼ æ–‡ä»¶æ•°é‡
                "failed_file_count": failed_count       # ä¸Šä¼ å¤±è´¥æ–‡ä»¶æ•°é‡
            }
            self.update_upload_eai_task_progress(final_progress_data)
            # print("hehe")

        
        # ç”±äºä½¿ç”¨äº†tqdmçš„positionå‚æ•°æ¥æ˜¾ç¤ºå¤šä¸ªè¿›åº¦æ¡
        # éœ€è¦é¢„å…ˆæ‰“å°è¶³å¤Ÿçš„ç©ºè¡Œä¸ºè¿›åº¦æ¡é¢„ç•™æ˜¾ç¤ºç©ºé—´
        print("\n" * (max_workers + 1))
        
        # åˆ›å»ºå…±äº«çš„æˆåŠŸå’Œå¤±è´¥æ–‡ä»¶åˆ—è¡¨
        shared_success_files = []
        shared_failed_files = []
        # åˆ›å»ºçº¿ç¨‹é”ä»¥ä¿æŠ¤å…±äº«åˆ—è¡¨
        lock = threading.Lock()
        
        # åˆ›å»ºåœæ­¢äº‹ä»¶ç”¨äºé€šçŸ¥è¿›åº¦æ›´æ–°çº¿ç¨‹åœæ­¢
        stop_event = threading.Event()
        
        # 2.å¯åŠ¨è¿›åº¦æ›´æ–°çº¿ç¨‹
        progress_thread = threading.Thread(
            target=update_progress_thread,
            args=(shared_success_files, shared_failed_files, lock, stop_event)
        )
        progress_thread.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼Œä¸»çº¿ç¨‹ç»“æŸæ—¶è‡ªåŠ¨ç»“æŸ
        progress_thread.start()
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œä¸Šä¼ 
        success_count = 0
        failure_count = 0
        success_files, failure_files = [],[]
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for i in range(max_workers):
                if thread_files[i]:  # åªä¸ºæœ‰æ–‡ä»¶çš„çº¿ç¨‹åˆ›å»ºä»»åŠ¡
                    future = executor.submit(
                        upload_task_thread,
                        i,  # thread_id
                        thread_files[i],  # è¯¥çº¿ç¨‹è´Ÿè´£çš„æ–‡ä»¶åˆ—è¡¨
                        thread_sizes[i],   # è¯¥çº¿ç¨‹è´Ÿè´£çš„æ–‡ä»¶æ€»å¤§å°
                        shared_success_files,  # å…±äº«çš„æˆåŠŸæ–‡ä»¶åˆ—è¡¨
                        shared_failed_files,   # å…±äº«çš„å¤±è´¥æ–‡ä»¶åˆ—è¡¨
                        lock  # çº¿ç¨‹é”
                    )
                    futures.append(future)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in futures:
                local_success, local_failed = future.result()
                success_count += len(local_success)
                success_files.extend(local_success)
                failure_count += len(local_failed)
                failure_files.extend(local_failed)

        
        # é€šçŸ¥è¿›åº¦æ›´æ–°çº¿ç¨‹åœæ­¢
        stop_event.set()
        # ç­‰å¾…è¿›åº¦æ›´æ–°çº¿ç¨‹å®Œæˆæœ€åä¸€æ¬¡æ›´æ–°
        progress_thread.join()
        
        # ç”±äºtqdmè¿›åº¦æ¡ä¼šå ç”¨ç»ˆç«¯ç©ºé—´
        # ä»»åŠ¡å®Œæˆåéœ€è¦æ‰“å°ç›¸åŒæ•°é‡çš„æ¢è¡Œæ¥"æ¸…ç†"è¿™äº›è¿›åº¦æ¡
        # å¦åˆ™åç»­è¾“å‡ºä¼šç´§è´´åœ¨è¿›åº¦æ¡ä¸Š
        print("\n" * (max_workers + 1))
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        print(f"\n{Fore.GREEN}æ‰¹é‡ä¸Šä¼ å®Œæˆ,ä¸Šä¼ ID:{self.eai_upload_task_id},å¯åœ¨æ•°é‡‡å¹³å°-æ•°æ®ç®¡ç†-SDKä¸Šä¼ æŸ¥çœ‹")
        print(f"{Fore.GREEN}æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
        if failure_count > 0:
            print(f"{Fore.RED}å¤±è´¥: {failure_count} ä¸ªæ–‡ä»¶ï¼Œæ–‡ä»¶åˆ—è¡¨:{failure_files}")
       
        # 3.é€šçŸ¥å…·èº«æ•°æ®å¹³å°å®Œæˆä¸Šä¼ 
        self.complete_upload_eai_task(status="SUCCESS")

 


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description=__description__)
    
    # æ ¸å¿ƒå‚æ•°
    parser.add_argument('-f', '--file', help='è¦ä¸Šä¼ çš„å•ä¸ªæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-d', '--directory', help='è¦ä¸Šä¼ çš„ç›®å½•è·¯å¾„')
    parser.add_argument('-s', '--dataset', help='ç›®æ ‡æ•°æ®é›†è·¯å¾„')
    
    # åŠŸèƒ½å¼€å…³
    parser.add_argument('--direct', action='store_true', help='ä½¿ç”¨ç›´æ¥è®¤è¯æ–¹å¼ï¼ˆä¸ä½¿ç”¨STSæœåŠ¡ï¼‰')
    parser.add_argument('--filter', help='æ–‡ä»¶è¿‡æ»¤å™¨ï¼Œç”¨é€—å·åˆ†éš”ï¼ˆä¾‹å¦‚ï¼š*.txt,*.csvï¼‰')
    parser.add_argument('--interactive', action='store_true', help='ä½¿ç”¨äº¤äº’æ¨¡å¼')
    
    return parser.parse_args()


def interactive_mode(use_direct_auth=False):
    """äº¤äº’æ¨¡å¼"""
    show_banner()
    
    # æ˜¾ç¤ºå½“å‰é…ç½®ä¿¡æ¯
    # config.print_config_info()
    # print()  # ç©ºè¡Œåˆ†éš”
    
    uploader = RobotDataUploader(use_direct_auth=use_direct_auth)
    
    # æ ¹æ®ç”¨æˆ·è¾“å…¥çš„AK/SKè·å–é‡‘å±±äº‘ks3çš„sts
    def get_sts():
        while True:
            ak = input(f"{Fore.BLUE}è¯·è¾“å…¥æ•°é‡‡å¹³å°Access Key: ")
            if not ak:
                print(f"{Fore.RED}é”™è¯¯: è¯·è¾“å…¥æ­£ç¡®çš„ak")
                continue
            sk = input(f"{Fore.BLUE}è¯·è¾“å…¥æ•°é‡‡å¹³å°Secret Key: ")
            if not sk:
                print(f"{Fore.RED}é”™è¯¯: è¯·è¾“å…¥æ­£ç¡®çš„sk")
                continue
            token = uploader.get_eai_token(ak, sk)
            if not token:
                print(f"{Fore.RED}é”™è¯¯: è®¤è¯å¤±è´¥,è¯·è¾“å…¥æ­£ç¡®çš„ak/sk")
                continue
            break
        while True:    
            is_success = uploader.get_ks3_sts()
            if not is_success:
                print(f"{Fore.RED}é”™è¯¯: è·å–å®‰å…¨å‡­è¯å¤±è´¥ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")
                continue
            break    
    
    # æ ¡éªŒä»»åŠ¡IDæ˜¯å¦å­˜åœ¨
    def get_task():
        while True:
            task_id = input(f"{Fore.BLUE}è¯·è¾“å…¥æ•°é‡‡å¹³å°å…³è”çš„ä»»åŠ¡ID {Fore.YELLOW}[å¯é€‰ï¼Œå›è½¦è·³è¿‡]: ")
            # è·³è¿‡
            if task_id is None or not task_id.strip():
                task_id = -99 
            is_success = uploader.get_eai_task(task_id)
            if not is_success:
                print(f"{Fore.RED}é”™è¯¯: æœªæ£€æµ‹åˆ°æœ‰å¯¹åº”çš„ä»»åŠ¡ID,è¯·æ£€æŸ¥")
                continue    
            break
    
    # æ ¡éªŒæºæ–‡ä»¶è·¯å¾„æœ‰æ•ˆæ€§
    def verify_source_file_path():
        while True:
            file_path = input(f"{Fore.BLUE}è¯·è¾“å…¥æ–‡ä»¶è·¯å¾„: ").strip('"')
            if not os.path.exists(file_path):
                print(f"{Fore.RED}é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ - {file_path}")
                continue
            if not os.path.isfile(file_path):
                print(f"{Fore.RED}é”™è¯¯ï¼šè·¯å¾„ä¸æ˜¯æ–‡ä»¶ - {file_path}")
                continue
            break
        return file_path
    
    # æ ¡éªŒæºç›®å½•è·¯å¾„æœ‰æ•ˆæ€§
    def verify_source_dir_path():
        while True:
            directory = input(f"{Fore.BLUE}è¯·è¾“å…¥å¾…ä¸Šä¼ ç›®å½•è·¯å¾„: ").strip('"')
            if not os.path.exists(directory):
                print(f"{Fore.RED}é”™è¯¯ï¼šç›®å½•ä¸å­˜åœ¨ - {directory}")
                continue
            if not os.path.isdir(directory):
                print(f"{Fore.RED}é”™è¯¯ï¼šè·¯å¾„ä¸æ˜¯ç›®å½• - {directory}")
                continue
            break
        return directory
            
    # æ ¡éªŒç›®æ ‡æ•°æ®è·¯å¾„æœ‰æ•ˆæ€§
    def verify_target_path():
        while True:
            sub_dir = input(f"{Fore.BLUE}è¯·è¾“å…¥ç›®æ ‡æ•°æ®é›†è·¯å¾„: ")
            if not sub_dir:
                print(f"{Fore.RED}é”™è¯¯ï¼šå¿…é¡»æŒ‡å®šç›®æ ‡æ•°æ®é›†è·¯å¾„")
                continue
            if not has_any_path_separator(sub_dir):
                print(f"{Fore.RED}é”™è¯¯: å¿…é¡»è‡³å°‘å«æœ‰1ä¸ªè·¯å¾„åˆ†éš”ç¬¦'/',å¦‚:a/b")
                continue
            break
        return sub_dir
    
    while True:
        show_menu(','.join(uploader.file_filters), 'ç›´æ¥è®¤è¯' if uploader.use_direct_auth else 'STSè®¤è¯', uploader.max_worker)
        # print(f"\n{Fore.CYAN}è¯·é€‰æ‹©æ“ä½œ:")
        # print(f"{Fore.WHITE}1. ä¸Šä¼ å•ä¸ªæ–‡ä»¶")
        # print(f"{Fore.WHITE}2. ä¸Šä¼ ç›®å½•")
        # print(f"{Fore.WHITE}3. è®¾ç½®æ–‡ä»¶è¿‡æ»¤å™¨ [{Fore.GREEN}å½“å‰: { ','.join(uploader.file_filters) }]")
        # print(f"{Fore.WHITE}4. åˆ‡æ¢è®¤è¯æ–¹å¼ [{Fore.GREEN}å½“å‰: {'ç›´æ¥è®¤è¯' if uploader.use_direct_auth else 'STSè®¤è¯'}]")
        # print(f"{Fore.WHITE}5. é€€å‡º")
        
        choice = input(f"{Fore.GREEN}> ")
        if choice == '1':
            get_sts()
            get_task()
            file_path = verify_source_file_path()
            sub_dir = verify_target_path()
            # ä¸Šä¼ å•ä¸ªæ–‡ä»¶
            start_time = time.time()
            uploader.upload_file(file_path, sub_dir)
            end_time = time.time()
            print(f"ä¸Šä¼ å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
        elif choice == '2':
            get_sts()
            get_task()
            directory = verify_source_dir_path()
            sub_dir = verify_target_path()
            # æ‰¹é‡ä¸Šä¼ æ–‡ä»¶
            start_time = time.time()
            uploader.batch_upload(directory, sub_dir)
            end_time = time.time()
            print(f"ä¸Šä¼ å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
        elif choice == '3':
            filters = input(f"{Fore.BLUE}è¯·è¾“å…¥æ–‡ä»¶è¿‡æ»¤å™¨ï¼ˆç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚: *.txt,*.csvï¼‰: ").split(',')
            uploader.set_file_filters(filters)
            print(f"{Fore.GREEN}æ–‡ä»¶è¿‡æ»¤å™¨å·²è®¾ç½®ä¸º: {', '.join(filters)}")
        elif choice == '4':
            uploader.use_direct_auth = not uploader.use_direct_auth
            uploader.connection = None  # é‡ç½®è¿æ¥
            print(f"{Fore.GREEN}å·²åˆ‡æ¢åˆ°{'ç›´æ¥è®¤è¯' if uploader.use_direct_auth else 'STSè®¤è¯'}æ–¹å¼")
        elif choice == '5':
            while True:
                try:
                    max_worker = input(f"{Fore.BLUE}è¯·è¾“å…¥å·¥ä½œçº¿ç¨‹æ•°ï¼š")
                    max_worker = int(max_worker)
                    import multiprocessing
                    cpu_count = multiprocessing.cpu_count()
                    if max_worker <= 0:
                        print(f"{Fore.RED}é”™è¯¯ï¼šå·¥ä½œçº¿ç¨‹æ•°å¿…é¡»å¤§äº0")
                        continue
                    if max_worker > cpu_count:
                        print(f"{Fore.YELLOW}è­¦å‘Šï¼šå·¥ä½œçº¿ç¨‹æ•°è¶…è¿‡CPUæ ¸å¿ƒæ•°({cpu_count})ï¼Œå¯èƒ½å½±å“æ€§èƒ½")
                        print(f"{Fore.YELLOW}è¯·é‡æ–°è¾“å…¥ä¸è¶…è¿‡{cpu_count}çš„çº¿ç¨‹æ•°")
                        continue
                    uploader.set_max_worker(max_worker)
                    print(f"{Fore.GREEN}å·¥ä½œçº¿ç¨‹æ•°å·²è®¾ç½®ä¸º: {max_worker}")
                    break
                except ValueError:
                    print(f"{Fore.RED}é”™è¯¯ï¼šè¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        elif choice == '6':
            break
        else:
            print(f"{Fore.RED}æ— æ•ˆé€‰æ‹©")


def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # å¦‚æœæ²¡æœ‰å‚æ•°æˆ–æŒ‡å®šäº†äº¤äº’æ¨¡å¼ï¼Œåˆ™è¿›å…¥äº¤äº’æ¨¡å¼
    if len(sys.argv) == 1 or args.interactive:
        interactive_mode(use_direct_auth=args.direct)
        return
    
    # å‘½ä»¤è¡Œæ¨¡å¼
    # æ˜¾ç¤ºå½“å‰é…ç½®ä¿¡æ¯
    # config.print_config_info()
    # print()  # ç©ºè¡Œåˆ†éš”
    
    uploader = RobotDataUploader(use_direct_auth=args.direct)
    
    # è®¾ç½®æ–‡ä»¶è¿‡æ»¤å™¨
    if args.filter:
        uploader.set_file_filters(args.filter.split(','))
    
    # æ£€æŸ¥å¿…éœ€å‚æ•°
    if not args.dataset:
        print(f"{Fore.RED}é”™è¯¯: å¿…é¡»æŒ‡å®šæ•°æ®é›†åç§° (-s æˆ– --dataset)")
        sys.exit(1)
    
    # ä¸Šä¼ æ–‡ä»¶æˆ–ç›®å½•
    start_time = time.time()
    if args.file:
        uploader.upload_file(args.file, args.dataset)
    elif args.directory:
        uploader.batch_upload(args.directory, args.dataset)
    end_time = time.time()
    print(f"ä¸Šä¼ å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")


if __name__ == '__main__':
    main() 